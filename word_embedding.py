# -*- coding: utf-8 -*-
"""word_embedding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QSjNII76wePeNmSn9mJ35oXzlL4LKreD
"""

!pip install tensorflow

import tensorflow as tf
print(tf.__version__)
print(tf.test.is_built_with_cuda())
print(tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None))

from tensorflow.keras.preprocessing.text import one_hot

sentences = [
    'My name is Suraj Keshari',
    'I am twenty years old',
    'I love machine learning',
    'I love deep learning too',
    'I want to be a good AI developer'
]

## vocabulary size
voc_size = 1000

onehot_repr = [one_hot(words, voc_size) for words in sentences]
print(onehot_repr)

from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential

#Pre padding
sent_max_length = 10
embeded_docs = pad_sequences(onehot_repr, padding='pre', maxlen=sent_max_length)
print(embeded_docs)

features = 10

model = Sequential()
model.add(Embedding(voc_size, features))
model.compile('adam', 'mse')

model.summary()

embeded_docs[0]

model.predict(embeded_docs[0])

